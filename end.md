# 毕业项目

实际上一边上课一边完成了一个内部服务的改造，相对于课程要求而言少了一些暂时不必要的东西。

## 微服务架构

整个服务目前仅仅是个 Job 模块，队列消费者，未来这里的开发经验会用于其他的项目。

## API 设计（包括 API 定义、错误码规范、Error 的使用）

目前采用的是 Probuf 定义了推送数据的全部字段，以实现具体的推送消息生成和高并发的推送实现的分离（目前所在公司的这块逻辑个性化程度很高，和一般的推送任务不太相似），每个模块专注于自己的事情。

错误码规范上，因为是消费者，所以不能直接将异常返回给生产者，内部的错误处理过程如下：

1. 错误的根因是 dao 层预定义的错误则按照规则执行不同的处理，并通过 statsD 统计报错的频次
2. 错误的根因是之前没有做过独立的异常处理的，则通过 Sentry 将异常统一收集，通过 Sentry 的 scope 机制返回自定义信息，Sentry 本身也会收集一些运行时信息，如 goroutine 数量，操作系统，代码的版本……

Error 的使用主要是在 dao 层对预定义的错误处理。

## gRPC 的使用

这块暂时没有使用，只是提供了一个简单 debug 性质的 HTTP API 利用 Probuf 生成的 Struct 作为 Post 数据反序列化的目标。

该 API 不用于生产性质的任务。

## Go 项目工程化（项目结构、DI、代码分层、ORM 框架）

DI 暂时没做，因为体量还很小，核心代码 1000 行不到。ORM 也没有用，目前用的的 NoSQL 数据库。

代码分层上受益颇多，这个项目在最开始的时候是有一个快糙猛的实现，所有的文件都在一个项目根目录。后来开始拆，

```
cmd/
└── app
    ├── main.go
    └── version.go
internal
├── app
│   └── app.go
├── dao
│   ├── apns
│   │   └── cilent.go
│   ├── doc.go
│   ├── err_codes.go
│   ├── mi
│   │   └── client.go
│   ├── mock
│   │   └── client.go
│   └── umeng
│       └── client.go
└── models
    ├── gc_task.pb.go
    ├── gc_task_utils.go
    ├── task.pb.go
    ├── task_test.go
    └── task_utils.go
pkg
├── config
│   └── config.go
├── consumer
│   ├── Classifier.go
│   ├── Pusher.go
│   ├── TaskBuffer.go
│   └── gc.go
├── controller
│   └── controller.go
├── monitor
│   └── report.go
└── server
    └── server.go
```

和各个推送服务接入的逻辑集成都在 `internal/dao/` 目录下，每个目录实际上提供 `Client`, `Notification` 这样的 strcut 定义，每个推送服务的接入都实现了 `ChannelClient` 的接口（接口的定义在别处），并且利用 `internal/dao/errors.go` 预定义的错误对底层的错误尽可能做了封装处理。

初始化后的推送客户端都在 `internal/app/app.go` 定一个 `App` 结构体的属性中，这个属性也提供了基于一定条件返回对应推送客户端的指针的方法。

进一步的，在 `pkg/consumer` 中，提供了任务读取、消费、异常处理、统计上报的功能，实际上在 `consumer` 模块中没有一行具体的推送实现的代码，都是接收 `internal/app/app.go` 返回的的接口，这样实现了程序控制与具体业务逻辑的分离，即「面向接口，而不是面向实现」。

后来发现这种设计非常好用，因为业务需要加个字段什么需求，只涉及 pb 文件，以及 dao 层的具体实现，其他的代码什么都不用改。

## 并发的使用（errgroup 的并行链路请求）

目前是一个 Goroutine 不断从 Redis List 取数据，然后放到进程内的 chan 中，由具体的消费者自己消费自己的推送数据。

## 微服务中间件的使用（ELK、Opentracing、Prometheus、Kafka）

没有使用 Kafka ，因为这个系统暂时没有多个生产者与消费者，1对1 的状态，不涉及任务的分流。目前看实际上分流的粒度不是消息生产者本身，而是推送基于任务数据的内部字段更合适，也在考虑拆分出不同的队列满足不同的需要。

Opentracing 在计划列表里，因为想分析下整个系统的性能，目前的 Go 这边的采样分析都是消费者，上游消费者是其他语言实现的，想了解从上游到下游的整个耗时全貌还是需要 Opentracing。

我们没有用 Prometheus 而是 statsd：通过周期性对系统采样上报，采集 Goroutine 数量&内存占用。之前这块做得不是很好，发生了同时海量 Goroutine 在运行，内存压力太大。也利用这个功能发现了很多之前没有想到的性能瓶颈。

## 缓存的使用优化（一致性处理、Pipeline 优化

暂时没有。

# 毕业总结

学这个课之前我是没有实际工程经验的，手头想用 Go 优化现有的项目困难重重，虽然有个 demo，但是远远不能提供实际的服务。学了很多工程经验之后，整个开发流程就通过多次重构理顺了。

工程化上主要的思想可以归纳成几个点：

- 面向协议（gRPC）
- 面向结构化的数据（Probuf）
- 面向接口，不要面向实现

这几点能保证业务逻辑本身的代码质量了，之前工作很多不顺的点就是老的项目有太多面条形的代码，每个步骤百来行的顺序处理过程，出了问题 debug 排查很麻烦，也很不顺。
虽然有痛点，但是没有解决的方式，也只能将就着。现在比较有信心慢慢解决这些问题了，期待将这些经验或者说范式迁移到其他的项目上。
